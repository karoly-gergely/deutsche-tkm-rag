# Backend Configuration
# Copy this file to .env and adjust values as needed

# Model Configuration
MODEL_ID=google/gemma-3-4b-it
DEV_MODEL_ID=google/gemma-3-1b-it
EMBEDDING_MODEL=intfloat/multilingual-e5-large

# Device Configuration
DEVICE=cuda

# Chunking Configuration
CHUNK_SIZE=800
CHUNK_OVERLAP=150

# Retrieval Configuration
TOP_K=5
RERANK_TOP_K=10
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Generation Configuration
MAX_CONTEXT_TOKENS=6000
MAX_NEW_TOKENS=768
TEMPERATURE=0.6
TOP_P=0.9

# Paths
DATA_FOLDER=data
CHROMA_DIR=.chroma
LOG_DIR=logs

# Logging Configuration
LOG_LEVEL=INFO
ENABLE_TRACING=false

# HuggingFace Hub Authentication
# Required for accessing private/gated models or higher rate limits
# Get your token from: https://huggingface.co/settings/tokens
# Leave empty if not using private models or if you're already logged in via CLI
HF_TOKEN=

# Development Mode (optional, auto-detected if not set)
# DEV_MODE=true
