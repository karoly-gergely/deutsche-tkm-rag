{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workflow & Backlog\n",
    "\n",
    "## Overview & Goals\n",
    "\n",
    "**Challenge:** Build a production-ready Retrieval-Augmented Generation (RAG) system for Deutsche Telekom that enables question-answering over internal publications and documents.\n",
    "\n",
    "**Goals:**\n",
    "- Efficiently index and retrieve relevant document chunks\n",
    "- Generate accurate, cited responses using retrieved context\n",
    "- Maintain traceability with publication IDs\n",
    "- Support enterprise requirements (security, monitoring, scalability)\n",
    "- Enable easy experimentation and iteration\n",
    "\n",
    "**Key Components:**\n",
    "- Document loading with metadata extraction\n",
    "- Metadata-aware chunking\n",
    "- Vector-based retrieval with optional reranking\n",
    "- LLM-based generation with citation\n",
    "- Structured logging and observability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Load and examine sample documents from the data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (notebooks/ is one level below project root)\n",
    "project_root = Path().resolve().parent\n",
    "if (project_root / \"config\").exists():\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    # Try current directory if already at project root\n",
    "    if (Path().resolve() / \"config\").exists():\n",
    "        sys.path.insert(0, str(Path().resolve()))\n",
    "\n",
    "from config import settings\n",
    "from loaders.loader import DocumentLoader\n",
    "\n",
    "# Load a few sample documents\n",
    "loader = DocumentLoader(data_folder=settings.DATA_FOLDER)\n",
    "documents = loader.load_all_documents()\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")\n",
    "print(\"\\nSample documents:\")\n",
    "for i, doc in enumerate(documents[:3], 1):\n",
    "    pub_id = doc.metadata.get(\"publication_id\", \"unknown\")\n",
    "    word_count = doc.metadata.get(\"word_count\", len(doc.page_content.split()))\n",
    "    print(f\"\\n{i}. Publication ID: {pub_id}\")\n",
    "    print(f\"   Word count: {word_count}\")\n",
    "    print(f\"   Topics: {doc.metadata.get('topics', [])}\")\n",
    "    print(f\"   Preview: {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word statistics across all documents\n",
    "if documents:\n",
    "    total_words = sum(doc.metadata.get(\"word_count\", 0) for doc in documents)\n",
    "    avg_words = total_words / len(documents) if documents else 0\n",
    "\n",
    "    print(\"\\nDocument Statistics:\")\n",
    "    print(f\"  Total documents: {len(documents)}\")\n",
    "    print(f\"  Total words: {total_words:,}\")\n",
    "    print(f\"  Average words per document: {avg_words:.0f}\")\n",
    "    print(f\"  Documents with topics: {sum(1 for d in documents if d.metadata.get('topics'))}\")\n",
    "    print(f\"  Documents with dates: {sum(1 for d in documents if d.metadata.get('mentioned_dates'))}\")\n",
    "\n",
    "    # Count by topics\n",
    "    all_topics = []\n",
    "    for doc in documents:\n",
    "        all_topics.extend(doc.metadata.get(\"topics\", []))\n",
    "\n",
    "    from collections import Counter\n",
    "    topic_counts = Counter(all_topics)\n",
    "    print(\"\\nTopic distribution:\")\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        print(f\"  {topic}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Workflow\n",
    "\n",
    "Index documents using the loader, chunker, and ChromaDB (in-memory for notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "from core.chunking import MetadataAwareChunker\n",
    "from core.embeddings import get_embeddings\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = get_embeddings()\n",
    "print(\"✓ Loaded embeddings model\")\n",
    "\n",
    "# Chunk documents with metadata\n",
    "chunker = MetadataAwareChunker(\n",
    "    chunk_size=settings.CHUNK_SIZE,\n",
    "    chunk_overlap=settings.CHUNK_OVERLAP\n",
    ")\n",
    "print(f\"✓ Initialized chunker (size={settings.CHUNK_SIZE}, overlap={settings.CHUNK_OVERLAP})\")\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "for doc in documents[:5]:  # Use first 5 docs for demo\n",
    "    source = doc.metadata.get(\"source\", \"Deutsche Telekom\")\n",
    "    doc_id = doc.metadata.get(\"publication_id\", doc.metadata.get(\"file_name\", \"unknown\"))\n",
    "    extra_metadata = {\n",
    "        k: v for k, v in doc.metadata.items()\n",
    "        if k not in [\"source\", \"publication_id\", \"file_name\"]\n",
    "    }\n",
    "\n",
    "    chunks = chunker.chunk_with_metadata(\n",
    "        text=doc.page_content,\n",
    "        source=source,\n",
    "        doc_id=doc_id,\n",
    "        **extra_metadata\n",
    "    )\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"✓ Created {len(all_chunks)} chunks from {min(5, len(documents))} documents\")\n",
    "print(f\"  Average chunks per document: {len(all_chunks) / min(5, len(documents)):.1f}\")\n",
    "\n",
    "# Filter complex metadata (lists, dicts, etc.) that ChromaDB doesn't support\n",
    "filtered_chunks = filter_complex_metadata(all_chunks)\n",
    "print(f\"✓ Filtered to {len(filtered_chunks)} chunks (removed complex metadata)\")\n",
    "\n",
    "# Show sample chunk metadata\n",
    "if filtered_chunks:\n",
    "    sample = filtered_chunks[0]\n",
    "    print(\"\\nSample chunk metadata:\")\n",
    "    print(f\"  chunk_id: {sample.metadata.get('chunk_id')}\")\n",
    "    print(f\"  doc_id: {sample.metadata.get('doc_id')}\")\n",
    "    print(f\"  chunk_index: {sample.metadata.get('chunk_index')}\")\n",
    "    print(f\"  total_chunks: {sample.metadata.get('total_chunks')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in-memory ChromaDB (no persistence for notebook)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=filtered_chunks,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"✓ Indexed {len(filtered_chunks)} chunks into ChromaDB\")\n",
    "print(f\"  Vector dimensions: {len(embeddings.embed_query('test'))}\")\n",
    "print(f\"  Collection count: {vectordb._collection.count() if hasattr(vectordb, '_collection') else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Demo\n",
    "\n",
    "Run queries and examine retrieved sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.retrieval import AdvancedRetriever\n",
    "\n",
    "# Create retriever (no reranker for CPU-only demo)\n",
    "retriever = AdvancedRetriever(vectordb=vectordb, reranker_model=None)\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"What is 5G?\",\n",
    "    \"Tell me about security\",\n",
    "    \"Partnership information\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    results = retriever.retrieve(query=query, top_k=3)\n",
    "\n",
    "    print(f\"Retrieved {len(results)} documents:\\n\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        pub_id = doc.metadata.get(\"publication_id\", doc.metadata.get(\"doc_id\", \"unknown\"))\n",
    "        chunk_idx = doc.metadata.get(\"chunk_index\", \"N/A\")\n",
    "        print(f\"{i}. [Publication: {pub_id}, Chunk: {chunk_idx}]\")\n",
    "        print(f\"   {doc.page_content[:150]}...\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Demo\n",
    "\n",
    "Build prompts and generate responses. Using mock model for CPU-only execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.prompt_manager import PromptManager\n",
    "\n",
    "prompt_manager = PromptManager()\n",
    "\n",
    "# Build RAG prompt with retrieved context\n",
    "query = \"What services does Deutsche Telekom offer?\"\n",
    "retrieved_docs = retriever.retrieve(query=query, top_k=2)\n",
    "\n",
    "prompt = prompt_manager.build_rag_prompt(\n",
    "    query=query,\n",
    "    context_docs=retrieved_docs,\n",
    "    chat_history=None\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(prompt[:1000])  # Show first 1000 chars\n",
    "print(\"...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrompt length: {len(prompt)} characters\")\n",
    "print(f\"Context documents used: {len(retrieved_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock generation for CPU-only demo (no actual LLM call)\n",
    "# In production, this would call generate_response() with real model\n",
    "\n",
    "print(\"Mock Generation Response:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract publication IDs from sources\n",
    "publication_ids = []\n",
    "for doc in retrieved_docs:\n",
    "    pub_id = doc.metadata.get(\"publication_id\", doc.metadata.get(\"doc_id\", \"unknown\"))\n",
    "    if pub_id not in publication_ids:\n",
    "        publication_ids.append(pub_id)\n",
    "\n",
    "# Simulate response (in real system, this comes from LLM)\n",
    "mock_response = f\"\"\"Based on the retrieved documents, Deutsche Telekom offers various telecommunications services including 5G network infrastructure, secure cloud solutions, and enterprise partnerships. The company focuses on expanding coverage and providing reliable connectivity services.\n",
    "\n",
    "Sources: {', '.join(publication_ids)}\"\"\"\n",
    "\n",
    "print(mock_response)\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[Note: This is a mock response. In production, use generate_response() with loaded model]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observability\n",
    "\n",
    "Show structured logging output example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from monitoring.logging import StructuredLogger\n",
    "\n",
    "# Create logger\n",
    "logger = StructuredLogger(\"notebook_demo\", log_dir=\"/tmp/notebook_logs\")\n",
    "\n",
    "# Log a sample query\n",
    "sample_docs = retrieved_docs[:2] if retrieved_docs else []\n",
    "logger.log_query(\n",
    "    query=\"What services does Deutsche Telekom offer?\",\n",
    "    retrieved_docs=sample_docs,\n",
    "    response_time=1.23,\n",
    "    user_id=\"notebook_user\"\n",
    ")\n",
    "\n",
    "# Show what was logged (read the JSON line from the log file)\n",
    "import os\n",
    "\n",
    "log_file = \"/tmp/notebook_logs/notebook_demo.jsonl\"\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file) as f:\n",
    "        log_line = f.readlines()[-1] if f.readlines() else None\n",
    "        if log_line:\n",
    "            log_data = json.loads(log_line)\n",
    "            print(\"Sample Structured Log Entry:\")\n",
    "            print(\"=\"*80)\n",
    "            print(json.dumps(log_data, indent=2))\n",
    "            print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Sample log entry structure:\")\n",
    "    sample_log = {\n",
    "        \"timestamp\": \"2024-01-01T12:00:00Z\",\n",
    "        \"level\": \"INFO\",\n",
    "        \"logger\": \"notebook_demo\",\n",
    "        \"message\": \"Query processed\",\n",
    "        \"event_type\": \"query\",\n",
    "        \"query\": \"What services does Deutsche Telekom offer?\",\n",
    "        \"num_documents\": 2,\n",
    "        \"response_time_seconds\": 1.23,\n",
    "        \"user_id\": \"notebook_user\",\n",
    "        \"document_ids\": [\"doc_1\", \"doc_2\"]\n",
    "    }\n",
    "    print(json.dumps(sample_log, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog of Ideas\n",
    "\n",
    "Future enhancements and experiments for the RAG system:\n",
    "\n",
    "### Retrieval & Ranking\n",
    "- **Switch to better embeddings**: Evaluate and switch to `intfloat/e5-large-v2` or `bge-large-en-v1.5` embeddings; compare recall@k metrics against current ~~`all-MiniLM-L6-v2`~~ `intfloat/multilingual-e5-large`\n",
    "  - *Implementation*: Update `core/embeddings.py` to support multiple embedding models with configurable selection via environment variables. Use `sentence-transformers` library to load models and benchmark against existing embeddings on a test query set.\n",
    "- **Max Marginal Relevance (MMR)**: Implement diversity-based retrieval to reduce redundant results\n",
    "  - *Implementation*: Use LangChain's `MMRRetriever` wrapper around ChromaDB, or implement custom MMR scoring that balances relevance (cosine similarity) with diversity (penalizes chunks similar to already-selected ones). Add lambda parameter to control diversity vs relevance tradeoff.\n",
    "- **Hybrid Retrieval**: Combine sparse+dense retrieval (BM25 + vector search) for improved recall\n",
    "  - *Implementation*: Integrate `rank-bm25` or `pyserini` for BM25 scoring. Combine BM25 and vector similarity scores using weighted fusion (e.g., Reciprocal Rank Fusion or weighted linear combination). Implement in `core/retrieval.py` as a new hybrid retriever class.\n",
    "- **Reranker A/B Testing**: Compare `cross-encoder/ms-marco-MiniLM-L-6-v2` vs `bge-reranker-large`; log reranking scores for analysis\n",
    "  - *Implementation*: Add reranker model selection via config, implement scoring logging in `AdvancedRetriever.retrieve()` to capture reranker scores for each candidate. Use feature flags or environment variables to switch models and compare metrics.\n",
    "\n",
    "### Provenance & Citations\n",
    "- **Normalized chunk IDs + provenance**: Store byte spans and filenames for each chunk\n",
    "  - *Implementation*: Extend `MetadataAwareChunker` to capture byte offsets during chunking. Store `start_byte`, `end_byte`, and `source_file` in chunk metadata. Update ingestion pipeline to preserve this information.\n",
    "- **Inline snippet highlights**: Show highlighted source snippets in responses with exact character spans\n",
    "  - *Implementation*: Parse citation references from LLM responses, match to chunk IDs, and extract the relevant text spans. Use frontend highlighting libraries (e.g., `react-highlight-words` or Streamlit markdown with HTML) to visually highlight matched text.\n",
    "\n",
    "### Response Quality & Guardrails\n",
    "- **Response guardrails**: \n",
    "  - Require citations for factual statements\n",
    "  - Refusal mechanism when confidence score is below threshold\n",
    "  - Log confidence scores for monitoring\n",
    "  - *Implementation*: Post-process LLM responses to check for citation markers (e.g., `[Source: ...]`). Implement confidence scoring using logit probabilities or self-consistency checks. Add configurable thresholds and refusal templates. Extend structured logging to capture confidence scores.\n",
    "- **Prompt compression (LLMLingua)**: Compress long context for better efficiency while preserving key information\n",
    "  - *Implementation*: Integrate `LLMLingua` or similar prompt compression libraries. Add compression step in `PromptManager.build_rag_prompt()` before sending to LLM. Use configurable compression ratios and preserve citation markers.\n",
    "\n",
    "### Evaluation & Quality\n",
    "- **Eval harness**: Implement RAGAS (Retrieval-Augmented Generation Assessment) metrics:\n",
    "  - Context Precision\n",
    "  - Context Recall  \n",
    "  - Faithfulness\n",
    "  - Answer Relevancy\n",
    "  - *Implementation*: Use `ragas` Python library to compute metrics. Create evaluation script that runs queries against golden dataset, compares retrieved contexts and generated answers, and outputs metrics JSON. Integrate with CI pipeline.\n",
    "- **Hand-curated Q/A set**: Create golden dataset over the 250 documents for regression testing\n",
    "  - *Implementation*: Create JSON/YAML format for Q/A pairs with expected answers and source documents. Store in `tests/golden_set/` directory. Include queries covering various topics and difficulty levels.\n",
    "- **Nightly CI evaluation**: Run RAGAS metrics in CI pipeline on golden set to detect regressions\n",
    "  - *Implementation*: Add GitHub Actions workflow or similar CI step that runs evaluation script, compares metrics against baseline thresholds, and fails build if metrics degrade. Store historical metrics for trend analysis.\n",
    "\n",
    "### Performance & Optimization\n",
    "- **Caching**: Implement Redis/SQLite caching layer keyed by `(normalized_query, embedding_hash, index_version)` to avoid redundant retrievals and generations\n",
    "  - *Implementation*: Create cache wrapper in `api/routes.py` that checks cache before retrieval/generation. Use query normalization (lowercase, remove punctuation) and hash embeddings to create stable cache keys. Store full responses with TTL. Use `cachetools` or `redis-py` for implementation.\n",
    "- **Vector DB swaps**: Support FAISS index via Chroma settings; create plug-in layer for Milvus/PGVector backends\n",
    "  - *Implementation*: Abstract vector store interface in `core/retrieval.py`. Implement adapter pattern for different vector stores. Use LangChain's vector store abstractions or create custom adapters. Make backend selection configurable via environment variables.\n",
    "- **Async Processing**: Use async/await for concurrent operations\n",
    "  - *Implementation*: Convert FastAPI endpoints to async functions. Use `asyncio.gather()` for parallel operations (e.g., concurrent document retrieval, parallel embedding generation). Update `AdvancedRetriever` to support async retrieval if needed.\n",
    "\n",
    "### User Experience\n",
    "- **Cancellation support**: Allow users to cancel long-running queries\n",
    "  - *Implementation*: Use FastAPI's `BackgroundTasks` with cancellation tokens or `asyncio.Task` cancellation. For streaming, implement `Server-Sent Events` with cancellation support. Add cancellation endpoint and frontend UI to trigger cancellation.\n",
    "- **Streaming support**: Add streaming response support in FastAPI endpoints for real-time token delivery\n",
    "  - *Implementation*: Use FastAPI's `StreamingResponse` with Server-Sent Events (SSE) format. Update `api/routes.py` to create a new `/query/stream` endpoint that yields tokens as they're generated and utilise `generate_stream()` method from `llm/generation.py` to generate them. Format each token as SSE event (`data: {token}\\n\\n`). Handle connection cleanup and error handling. Update frontend (React) to consume SSE stream and display tokens progressively using `EventSource` API or `sseclient` library.\n",
    "- **Queue limit**: Implement request queue limits to prevent overload\n",
    "  - *Implementation*: Use `asyncio.Semaphore` or `slowapi` rate limiter to cap concurrent requests. Add queue management with configurable max queue size. Return HTTP 503 when queue is full.\n",
    "\n",
    "### Security & Authentication\n",
    "- **FastAPI authentication**: Implement API key or OAuth2 authentication\n",
    "  - *Implementation*: Use `fastapi-users` or `python-jose` for JWT tokens. Implement API key middleware that validates keys from environment variables or database. Add authentication decorators to protected endpoints.\n",
    "- **Rate limiting**: Add per-user/IP rate limiting to prevent abuse\n",
    "  - *Implementation*: Use `slowapi` library with `Limiter` middleware. Configure rate limits per user/IP combination. Store rate limit state in Redis or in-memory cache. Add rate limit headers to responses.\n",
    "- **SSL/TSL**: Setup Caddy to serve both Streamlit and FastAPI via https (instead of self-signed for FastAPI and http for Streamlit)\n",
    "  - *Implementation*: Configure Caddy reverse proxy with automatic HTTPS (Let's Encrypt). Update `docker-compose.yml` to include Caddy container. Route traffic: Caddy → Streamlit (port 8501) and Caddy → FastAPI (port 8080). Update CORS settings to allow Caddy domain.\n",
    "\n",
    "### Multilingual & Internationalization\n",
    "- **Multilingual Support**: Support documents in multiple languages with language detection\n",
    "  - *Implementation*: Use `langdetect` or `fasttext` for language detection during document ingestion. Store language metadata in chunk metadata. Use language-specific embedding models (e.g., multilingual sentence transformers) or language-specific rerankers.\n",
    "- **Cross-lingual Retrieval**: Retrieve relevant content even when query language differs from document language\n",
    "  - *Implementation*: Use multilingual embedding models (e.g., `paraphrase-multilingual-MiniLM-L12-v2`) that support cross-lingual similarity. Alternatively, use translation APIs to translate queries to document language before retrieval.\n",
    "- **React Interface Multi-lingual**: Add i18n for the React consumer and move hard-coded strings to lang files\n",
    "  - *Implementation*: Use `react-i18next` or `i18n` library. Create translation files (JSON/YAML) for each language in `react/src/locales/`. Extract all user-facing strings and replace with translation keys. Add language selector UI component.\n",
    "\n",
    "### Data Management & Deployment\n",
    "\n",
    "- **Externalize data folder**: Inject the data/ directory at runtime instead of committing it to Git:\n",
    "  - Mount it as a Docker/K8s volume or download from secure storage (S3, Azure Blob, etc.) during startup.\n",
    "  - Add checksum validation and update scripts to ensure dataset integrity across environments.\n",
    "  - *Implementation*: Update `docker-compose.yml` to mount external volume for `data/` directory. Add startup script that downloads data from S3/Azure if `DATA_SOURCE_URL` is set. Use `boto3` or `azure-storage-blob` for cloud storage. Implement checksum validation using SHA256 hashes stored in metadata file."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
